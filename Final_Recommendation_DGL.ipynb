{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Final_Recommendation_DGL.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2Sq86KSuf0o"
      },
      "source": [
        "# Recommender Systems with DGL\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Graph Neural Networks (GNN), as a methodology of learning representations on graphs, has gained much attention recently.  Various models such as Graph Convolutional Networks, GraphSAGE, etc. are proposed to obtain representations of whole graphs, or nodes on a single graph.\n",
        "\n",
        "A primary goal of recommendation is to automatically make predictions about a user's interest, e.g. whether/how a user would interact with a set of items, given the interaction history of the user herself, as well as the histories of other users.  The user-item interaction can also be viewed as a bipartite graph, where users and items form two sets of nodes, and edges connecting them stands for interactions.  The problem can then be formulated as a *link-prediction* problem, where we try to predict whether an edge (of a given type) exists between two nodes.\n",
        "\n",
        "Based on this intuition, the academia developed multiple new models for recommendation, including but not limited to:\n",
        "\n",
        "* Geometric Learning Approaches\n",
        "  * [Geometric Matrix Completion](https://papers.nips.cc/paper/5938-collaborative-filtering-with-graph-information-consistency-and-scalable-methods.pdf)\n",
        "  * [Recurrent Multi-graph CNN](https://arxiv.org/pdf/1704.06803.pdf)\n",
        "* Graph-convolutional Approaches\n",
        "  * Models such as [R-GCN](https://arxiv.org/pdf/1703.06103.pdf) or [GraphSAGE](https://github.com/stellargraph/stellargraph/tree/develop/demos/link-prediction/hinsage) also apply.\n",
        "  * [Graph Convolutional Matrix Completion](https://arxiv.org/abs/1706.02263)\n",
        "  * [PinSage](https://arxiv.org/pdf/1806.01973.pdf)\n",
        "  \n",
        "In this hands-on tutorial, we will demonstrate how to write a recommendation model with GraphSAGE in DGL + MXNet. We just demonstrate rating prediction in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrKS8kxCulB2",
        "outputId": "0b73cae2-1fab-4e59-ad97-4e4580ece214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "!pip install dgl==0.4.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl==0.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/07/978ee25929b35e1e9c8165407683e3f4f1565a75f699a6da7cb48bf492d8/dgl-0.4.3-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from dgl==0.4.3) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl==0.4.3) (1.18.5)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl==0.4.3) (2.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl==0.4.3) (1.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl==0.4.3) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl==0.4.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl==0.4.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl==0.4.3) (2.10)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl==0.4.3) (4.4.2)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnyISK1cu1C6",
        "outputId": "521da751-b2f9-4568-febe-6f1efc6af124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!pip install mxnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/bb/54cbabe428351c06d10903c658878d29ee7026efbe45133fd133598d6eb6/mxnet-1.7.0.post1-py2.py3-none-manylinux2014_x86_64.whl (55.0MB)\n",
            "\u001b[K     |████████████████████████████████| 55.0MB 74kB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.18.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.7.0.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdF5V-DMuf0r"
      },
      "source": [
        "import dgl\n",
        "import dgl.function as FN\n",
        "\n",
        "# Load MXNet as backend\n",
        "dgl.load_backend('mxnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f1VJguVuf00"
      },
      "source": [
        "import mxnet as mx\n",
        "from mxnet import ndarray as nd, autograd, gluon\n",
        "from mxnet.gluon import nn\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtwTCK40uf06"
      },
      "source": [
        "## Loading data\n",
        "\n",
        "In this tutorial, we focus on rating prediction on MovieLens-100K dataset.  The data comes from [MovieLens](http://files.grouplens.org/datasets/movielens/ml-1m.zip) and is shipped with the notebook already.\n",
        "\n",
        "The dataset is encapsulated in `movielens.MovieLens` class for clarity of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKqEm9pSvIfi"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from functools import partial\n",
        "\n",
        "class MovieLens(object):\n",
        "    split_by_time = None\n",
        "    \n",
        "    def read_user_line(self, l):\n",
        "        id_, age, gender, occupation, zip_ = l.strip().split('|')\n",
        "        age = np.searchsorted([20, 30, 40, 50, 60], age)   # bin the ages into <20, 20-30, 30-40, ..., >60\n",
        "        return {'id': int(id_), 'gender': gender, 'age': age, 'occupation': occupation, 'zip': zip_}\n",
        "    \n",
        "    def read_product_line(self, l):\n",
        "        fields = l.strip().split('|')\n",
        "        id_ = fields[0]\n",
        "        title = fields[1]\n",
        "        genres = fields[-19:]\n",
        "\n",
        "        # extract year\n",
        "        if re.match(r'.*\\([0-9]{4}\\)$', title):\n",
        "            year = title[-5:-1]\n",
        "            title = title[:-6].strip()\n",
        "        else:\n",
        "            year = 0\n",
        "\n",
        "        data = {'id': int(id_), 'title': title, 'year': year}\n",
        "        for i, g in enumerate(genres):\n",
        "            g = int(g)\n",
        "            data['genre' + str(i)] = (g != 0)\n",
        "        return data\n",
        "    \n",
        "    def read_rating_line(self, l):\n",
        "        user_id, product_id, rating, timestamp = l.split()\n",
        "        return {'user_id': int(user_id), 'product_id': int(product_id), 'rating': float(rating), 'timestamp': int(timestamp)}\n",
        "    \n",
        "    def __init__(self, directory):\n",
        "        '''\n",
        "        directory: path to movielens directory which should have the three\n",
        "                   files:\n",
        "                   users.dat\n",
        "                   products.dat\n",
        "                   ratings.dat\n",
        "        '''\n",
        "        self.directory = directory\n",
        "\n",
        "        users = []\n",
        "        products = []\n",
        "        ratings = []\n",
        "\n",
        "        # read ratings\n",
        "        with open(os.path.join(directory, 'ua.base')) as f:\n",
        "            for l in f:\n",
        "                rating = self.read_rating_line(l)\n",
        "                ratings.append(rating)\n",
        "        with open(os.path.join(directory, 'ua.test')) as f:\n",
        "            for l in f:\n",
        "                rating = self.read_rating_line(l)\n",
        "                ratings.append(rating)\n",
        "                \n",
        "        ratings = pd.DataFrame(ratings)\n",
        "        product_count = ratings['product_id'].value_counts()\n",
        "        product_count.name = 'product_count'\n",
        "        ratings = ratings.join(product_count, on='product_id')\n",
        "        self.ratings = ratings\n",
        "\n",
        "        # read users - if user feature does not exist, we find all unique user IDs\n",
        "        # appeared in the rating table and create an empty table from that.\n",
        "        user_file = os.path.join(directory, 'u.user')\n",
        "        with open(user_file) as f:\n",
        "            for l in f:\n",
        "                users.append(self.read_user_line(l))\n",
        "        self.users = pd.DataFrame(users).set_index('id').astype('category')\n",
        "\n",
        "        # read products\n",
        "        with open(os.path.join(directory, 'u.item'), encoding='latin1') as f:\n",
        "            for l in f:\n",
        "                products.append(self.read_product_line(l))\n",
        "        self.products = (\n",
        "                pd.DataFrame(products)\n",
        "                .set_index('id')\n",
        "                .astype({'year': 'category'}))\n",
        "        self.genres = self.products.columns[self.products.dtypes == bool]\n",
        "        \n",
        "        self.ratings = self.data_split(self.ratings)\n",
        "        self.users = self.users[self.users.index.isin(self.ratings['user_id'])]\n",
        "        self.products = self.products[self.products.index.isin(self.ratings['product_id'])]\n",
        "        \n",
        "        self.build_graph()\n",
        "        self.generate_mask()\n",
        "        self.generate_candidates()\n",
        "        \n",
        "        # Construct the training subgraph.\n",
        "        train_eid = self.g.filter_edges(lambda edges: edges.data['train']).astype('int64')\n",
        "        self.g_train = self.g.edge_subgraph(train_eid, preserve_nodes=True)\n",
        "        self.g_train.copy_from_parent()\n",
        "\n",
        "    # Change this field to 'timestamp' to perform a user-based temporal split of the dataset,\n",
        "    # where the ratings of each users are ordered by timestamps.  The first 80% of the ordered\n",
        "    # data falls into the training set, and the middle 10% and last 10% goes to validation and\n",
        "    # test set respectively.\n",
        "    # If this field is None, the ratings of each users would be shuffled and splitted in a\n",
        "    # ratio of 80-10-10.\n",
        "    split_by_time = None\n",
        "    def split_user(self, df, filter_counts=0, timestamp=None):\n",
        "        df_new = df.copy()\n",
        "        df_new['prob'] = -1\n",
        "\n",
        "        df_new_sub = (df_new['product_count'] >= filter_counts).to_numpy().nonzero()[0]\n",
        "        prob = np.linspace(0, 1, df_new_sub.shape[0], endpoint=False)\n",
        "        if timestamp is not None and timestamp in df_new.columns:\n",
        "            df_new = df_new.iloc[df_new_sub].sort_values(timestamp)\n",
        "            df_new['prob'] = prob\n",
        "            return df_new\n",
        "        else:\n",
        "            np.random.shuffle(prob)\n",
        "            df_new['prob'].iloc[df_new_sub] = prob\n",
        "            return df_new\n",
        "\n",
        "    def data_split(self, ratings):\n",
        "        ratings = ratings.groupby('user_id', group_keys=False).apply(\n",
        "                partial(self.split_user, filter_counts=10, timestamp=self.split_by_time))\n",
        "        ratings['train'] = ratings['prob'] <= 0.8\n",
        "        ratings['valid'] = (ratings['prob'] > 0.8) & (ratings['prob'] <= 0.8)\n",
        "        ratings['test'] = ratings['prob'] > 0.8\n",
        "        ratings.drop(['prob'], axis=1, inplace=True)\n",
        "        return ratings\n",
        "\n",
        "\n",
        "    # process the features and build the DGL graph\n",
        "    def build_graph(self):\n",
        "        import mxnet as mx\n",
        "        from mxnet import ndarray as nd\n",
        "        from mxnet import gluon, autograd\n",
        "        import dgl\n",
        "        \n",
        "        user_ids = list(self.users.index)\n",
        "        product_ids = list(self.products.index)\n",
        "        user_ids_invmap = {id_: i for i, id_ in enumerate(user_ids)}\n",
        "        product_ids_invmap = {id_: i for i, id_ in enumerate(product_ids)}\n",
        "        self.user_ids = user_ids\n",
        "        self.product_ids = product_ids\n",
        "        self.user_ids_invmap = user_ids_invmap\n",
        "        self.product_ids_invmap = product_ids_invmap\n",
        "\n",
        "        g = dgl.DGLGraph(multigraph=True)\n",
        "        g.add_nodes(len(user_ids) + len(product_ids))\n",
        "\n",
        "        # node type\n",
        "        node_type = nd.zeros(g.number_of_nodes(), dtype='float32')\n",
        "        node_type[:len(user_ids)] = 1\n",
        "        g.ndata['type'] = node_type\n",
        "\n",
        "        # user features\n",
        "        print('Adding user features...')\n",
        "        for user_column in self.users.columns:\n",
        "            udata = nd.zeros(g.number_of_nodes(), dtype='int64')\n",
        "            # 0 for padding\n",
        "            udata[:len(user_ids)] = \\\n",
        "                    nd.from_numpy(self.users[user_column].cat.codes.values.astype('int64') + 1)\n",
        "            g.ndata[user_column] = udata\n",
        "\n",
        "        # product genre\n",
        "        print('Adding product features...')\n",
        "        product_genres = nd.from_numpy(self.products[self.genres].values.copy().astype('float32'))\n",
        "        g.ndata['genre'] = nd.zeros((g.number_of_nodes(), len(self.genres)))\n",
        "        g.ndata['genre'][len(user_ids):len(user_ids) + len(product_ids)] = product_genres\n",
        "\n",
        "        # product year\n",
        "        if 'year' in self.products.columns:\n",
        "            g.ndata['year'] = nd.zeros(g.number_of_nodes(), dtype='int64')\n",
        "            # 0 for padding\n",
        "            g.ndata['year'][len(user_ids):len(user_ids) + len(product_ids)] = \\\n",
        "                    nd.from_numpy(self.products['year'].cat.codes.values.astype('int64') + 1)\n",
        "\n",
        "        '''\n",
        "        # product title\n",
        "        print('Parsing title...')\n",
        "        nlp = stanfordnlp.Pipeline(use_gpu=False, processors='tokenize,lemma')\n",
        "        vocab = set()\n",
        "        title_words = []\n",
        "        for t in tqdm.tqdm(self.products['title'].values):\n",
        "            doc = nlp(t)\n",
        "            words = set()\n",
        "            for s in doc.sentences:\n",
        "                words.update(w.lemma.lower() for w in s.words\n",
        "                             if not re.fullmatch(r'['+string.punctuation+']+', w.lemma))\n",
        "            vocab.update(words)\n",
        "            title_words.append(words)\n",
        "        vocab = list(vocab)\n",
        "        vocab_invmap = {w: i for i, w in enumerate(vocab)}\n",
        "        # bag-of-words\n",
        "        g.ndata['title'] = nd.zeros((g.number_of_nodes(), len(vocab)))\n",
        "        for i, tw in enumerate(tqdm.tqdm(title_words)):\n",
        "            g.ndata['title'][len(user_ids) + i, [vocab_invmap[w] for w in tw]] = 1\n",
        "        self.vocab = vocab\n",
        "        self.vocab_invmap = vocab_invmap\n",
        "        '''\n",
        "        \n",
        "        rating_user_vertices = [user_ids_invmap[id_] for id_ in self.ratings['user_id'].values]\n",
        "        rating_product_vertices = [product_ids_invmap[id_] + len(user_ids)\n",
        "                                 for id_ in self.ratings['product_id'].values]\n",
        "        self.rating_user_vertices = rating_user_vertices\n",
        "        self.rating_product_vertices = rating_product_vertices\n",
        "\n",
        "        g.add_edges(\n",
        "                rating_user_vertices,\n",
        "                rating_product_vertices,\n",
        "                data={'inv': nd.zeros(self.ratings.shape[0], dtype='int32'),\n",
        "                    'rating': nd.from_numpy(self.ratings['rating'].values.astype('float32'))})\n",
        "        g.add_edges(\n",
        "                rating_product_vertices,\n",
        "                rating_user_vertices,\n",
        "                data={'inv': nd.ones(self.ratings.shape[0], dtype='int32'),\n",
        "                    'rating': nd.from_numpy(self.ratings['rating'].values.astype('float32'))})\n",
        "        self.g = g\n",
        "        g.readonly()\n",
        "        \n",
        "    # Assign masks of training, validation and test set onto the DGL graph\n",
        "    # according to the rating table.\n",
        "    def generate_mask(self):\n",
        "        from mxnet import ndarray as nd\n",
        "        valid_tensor = nd.from_numpy(self.ratings['valid'].values.astype('int32'))\n",
        "        test_tensor = nd.from_numpy(self.ratings['test'].values.astype('int32'))\n",
        "        train_tensor = nd.from_numpy(self.ratings['train'].values.astype('int32'))\n",
        "        edge_data = {\n",
        "                'valid': valid_tensor,\n",
        "                'test': test_tensor,\n",
        "                'train': train_tensor,\n",
        "                }\n",
        "\n",
        "        self.g.edges[self.rating_user_vertices, self.rating_product_vertices].data.update(edge_data)\n",
        "        self.g.edges[self.rating_product_vertices, self.rating_user_vertices].data.update(edge_data)\n",
        "        \n",
        "    # Generate the list of products for each user in training/validation/test set.\n",
        "    def generate_candidates(self):\n",
        "        self.p_train = []\n",
        "        self.p_valid = []\n",
        "        self.p_test = []\n",
        "        for uid in self.user_ids:\n",
        "            user_ratings = self.ratings[self.ratings['user_id'] == uid]\n",
        "            self.p_train.append(np.array(\n",
        "                [self.product_ids_invmap[i] for i in user_ratings[user_ratings['train']]['product_id'].values]))\n",
        "            self.p_valid.append(np.array(\n",
        "                [self.product_ids_invmap[i] for i in user_ratings[user_ratings['valid']]['product_id'].values]))\n",
        "            self.p_test.append(np.array(\n",
        "                [self.product_ids_invmap[i] for i in user_ratings[user_ratings['test']]['product_id'].values]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4REz3oIzvSpi",
        "outputId": "f56ccd17-c1c8-49db-8c17-13b5970dd739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/dglai/KDD-2019-Hands-on.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'KDD-2019-Hands-on'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 273 (delta 3), reused 0 (delta 0), pack-reused 265\u001b[K\n",
            "Receiving objects: 100% (273/273), 19.78 MiB | 22.86 MiB/s, done.\n",
            "Resolving deltas: 100% (148/148), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMXXIXrBuf07",
        "outputId": "c791e085-7c66-4f9d-b0e0-1e8c89acdff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "ml = MovieLens('/content/KDD-2019-Hands-on/3_recommender_system/ml-100k')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/dgl/base.py:25: UserWarning: multigraph will be deprecated.DGL will treat all graphs as multigraph in the future.\n",
            "  warnings.warn(msg, warn_type)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding user features...\n",
            "Adding product features...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otfxYyYYuf1A"
      },
      "source": [
        "The MovieLens graph has 2,625 nodes and 200,000 edges. The graph contains both user nodes and movie nodes. The MovieLens dataset contains 100,000 ratings and the ratings are stored as edges. DGL only supports directed edges. Thus, each rating is stored twice: one connects from a user node to a movie node, and the other connects from a movie node to a user node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q37HUf9Cuf1B",
        "outputId": "364ae246-7e2c-4660-82cd-80afec604fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# This is a DGLGraph object.\n",
        "g = ml.g\n",
        "print('#vertices:', g.number_of_nodes())\n",
        "print('#edges:', g.number_of_edges())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#vertices: 2625\n",
            "#edges: 200000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJN3SHz1uf1J"
      },
      "source": [
        "## See the features in the MovieLens dataset\n",
        "\n",
        "The MovieLens dataset has some user features and movie features.\n",
        "\n",
        "User features:\n",
        "* age,\n",
        "* gender,\n",
        "* occupation,\n",
        "* zip code,\n",
        "\n",
        "Movie features:\n",
        "* genre,\n",
        "* year,\n",
        "\n",
        "We use one-hot encoding for \"age\", \"gender\", \"occupation\", \"zip code\" and \"year\". \"genre\" uses multi-hot encoding. We then store them as node features on the graph.\n",
        "\n",
        "Since user features and item features are different, we pad both types of features with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F8XVLSHuf1K",
        "outputId": "cbb4a207-047d-4ddf-ed8f-f428479a521d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "print(g.ndata)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'type': \n",
            "[1. 1. 1. ... 0. 0. 0.]\n",
            "<NDArray 2625 @cpu(0)>, 'gender': \n",
            "[2 1 2 ... 0 0 0]\n",
            "<NDArray 2625 @cpu(0)>, 'age': \n",
            "[2 5 2 ... 0 0 0]\n",
            "<NDArray 2625 @cpu(0)>, 'occupation': \n",
            "[20 14 21 ...  0  0  0]\n",
            "<NDArray 2625 @cpu(0)>, 'zip': \n",
            "[623 690 271 ...   0   0   0]\n",
            "<NDArray 2625 @cpu(0)>, 'genre': \n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "<NDArray 2625x19 @cpu(0)>, 'year': \n",
            "[ 0  0  0 ... 72 68 65]\n",
            "<NDArray 2625 @cpu(0)>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWyARR1Muf1Q"
      },
      "source": [
        "There is a node data \"type\" that indicates the node type in the bipartite graph. Nodes with \"type=1\" are user nodes and \"type=0\" are movie nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlzFi4Niuf1R",
        "outputId": "bdf9cb95-c8a1-44e0-ca68-d0811bfdb6a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('#user nodes:', mx.nd.sum(g.ndata['type'] == 1).asnumpy())\n",
        "print('#movie nodes:', mx.nd.sum(g.ndata['type'] == 0).asnumpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#user nodes: [943.]\n",
            "#movie nodes: [1682.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEiqY7Owuf1Y"
      },
      "source": [
        "# Recommendation model with GNN\n",
        "\n",
        "<img src=\"https://s3.us-east-2.amazonaws.com/dgl.ai/amlc_tutorial/rec_process.png\" width=\"800\">\n",
        "\n",
        "Recommendation with graph neural networks has two steps:\n",
        "* graph encoder: use graph neural networks to compute node embeddings.\n",
        "* edge decoder: compute scores on edges with user embeddings and movie embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnLfnz3tuf1a"
      },
      "source": [
        "class GNNRecommender(nn.Block):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GNNRecommender, self).__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, G, users, items):\n",
        "        h = self.encoder(G)\n",
        "        h_users = h[users]\n",
        "        h_items = h[items]\n",
        "        return self.decoder(h_users, h_items, users, items)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u12ZRyimuf1g"
      },
      "source": [
        "## GraphSage encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHPg1TFOuf1h"
      },
      "source": [
        "The encoder does two things:\n",
        "* generate the initial user and movie embeddings from categorial features.\n",
        "* run GraphSAGE layers on the MovieLens graph multiple times to compute the final embeddings for rating prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DkYfrnNuf1i"
      },
      "source": [
        "class GraphSageEncoder(nn.Block):\n",
        "    def __init__(self, embedding_size, n_layers, G):\n",
        "        super(GraphSageEncoder, self).__init__()\n",
        "\n",
        "        self.G = G\n",
        "        self.user_nodes = G.filter_nodes(lambda nodes: nodes.data['type'] == 1).astype(np.int64)\n",
        "        self.movie_nodes = G.filter_nodes(lambda nodes: nodes.data['type'] == 0).astype(np.int64)\n",
        "\n",
        "        self.layers = nn.Sequential()\n",
        "        for i in range(n_layers):\n",
        "            self.layers.add(GraphSageLayer(embedding_size, G, self.user_nodes, self.movie_nodes))\n",
        "\n",
        "        # One-hot encoding for each node.\n",
        "        node_emb = nn.Embedding(G.number_of_nodes() + 1, embedding_size)\n",
        "        self.user_emb = UserEmbedding(G, embedding_size, node_emb)\n",
        "        self.movie_emb = MovieEmbedding(G, embedding_size, node_emb)\n",
        "\n",
        "    def forward(self, G):\n",
        "        # Generate embeddings on user nodes and movie nodes.\n",
        "        assert G.number_of_nodes() == self.G.number_of_nodes()\n",
        "        G.apply_nodes(lambda nodes: {'h': self.user_emb(nodes.data, self.user_nodes)}, self.user_nodes)\n",
        "        G.apply_nodes(lambda nodes: {'h': self.movie_emb(nodes.data, self.movie_nodes)}, self.movie_nodes)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            layer(G)\n",
        "\n",
        "        # The node embeddings computed by GraphSage layers are stored in node data of 'h'.\n",
        "        return G.ndata['h']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZU6Mma8uf1m"
      },
      "source": [
        "## GraphSage model\n",
        "\n",
        "We can now write a GraphSAGE layer.  In GraphSAGE, the node representation is updated with the representation in the previous layer as well as an aggregation (often mean) of \"messages\" sent from all neighboring nodes.\n",
        "\n",
        "### Algorithm\n",
        "\n",
        "The algorithm of a single GraphSAGE layer has three steps for each node $v$:\n",
        "\n",
        "1. $h_{\\mathcal{N}(v)} \\gets \\mathtt{Sum}_{u \\in \\mathcal{N}(v)} h_{u}$\n",
        "2. $h_{v} \\gets \\sigma\\left(W \\cdot \\mathtt{CONCAT}(h_v, h_{\\mathcal{N}(v)}/d_{\\mathcal{N}(v)})\\right)$\n",
        "3. $h_{v} \\gets h_{v} / \\lVert h_{v} \\rVert_2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxTOMtfsuf1o"
      },
      "source": [
        "### Slight modification on the original GraphSage model\n",
        "\n",
        "In practice, the MovieLens dataset has two types of nodes: users and movies. We need to perform separate node update functions on the two types of nodes on step 2.\n",
        "\n",
        "For the movie nodes,\n",
        "\n",
        "2. $h_{m} \\gets \\sigma\\left(W0 \\cdot \\mathtt{CONCAT}(h_m, h_{\\mathcal{N}(m)} / d_{\\mathcal{N}(m)})\\right)$\n",
        "\n",
        "For the user nodes,\n",
        "\n",
        "2. $h_{u} \\gets \\sigma\\left(W1 \\cdot \\mathtt{CONCAT}(h_u, h_{\\mathcal{N}(u)} / d_{\\mathcal{N}(u)})\\right)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wy0E6xmuf1p"
      },
      "source": [
        "class GraphSageLayer(nn.Block):\n",
        "    def __init__(self, feature_size, G, user_nodes, movie_nodes):\n",
        "        super(GraphSageLayer, self).__init__()\n",
        "\n",
        "        self.feature_size = feature_size\n",
        "        self.G = G\n",
        "        self.user_nodes = user_nodes\n",
        "        self.movie_nodes = movie_nodes\n",
        "\n",
        "        self.user_update = GraphSageNodeUpdate(feature_size)\n",
        "        self.movie_update = GraphSageNodeUpdate(feature_size)\n",
        "\n",
        "        all_nodes = mx.nd.arange(G.number_of_nodes(), dtype=np.int64)\n",
        "        self.deg = G.in_degrees(all_nodes).astype(np.float32)\n",
        "\n",
        "    def forward(self, G):\n",
        "        assert G.number_of_nodes() == self.G.number_of_nodes()\n",
        "        G.ndata['deg'] = self.deg\n",
        "        # Step 1\n",
        "        # The message function and message reduce function used by GraphSage.\n",
        "        # EXERCISE: Interested users can try different message functions and reduce functions.\n",
        "        G.update_all(FN.copy_src('h', 'h'), FN.sum('h', 'h_agg'))\n",
        "        # Step 2 and 3\n",
        "        G.apply_nodes(self.user_update, self.user_nodes)\n",
        "        G.apply_nodes(self.movie_update, self.movie_nodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKXeGp9Guf1u"
      },
      "source": [
        "class GraphSageNodeUpdate(nn.Block):\n",
        "    def __init__(self, feature_size):\n",
        "        super(GraphSageNodeUpdate, self).__init__()\n",
        "\n",
        "        self.feature_size = feature_size\n",
        "        self.W = nn.Dense(feature_size)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, nodes):\n",
        "        # Node embedding from the previous layer.\n",
        "        h = nodes.data['h']\n",
        "        # Aggregation of the node embeddings in the neighborhood\n",
        "        h_agg = nodes.data['h_agg']\n",
        "        # Degree of the vertex.\n",
        "        deg = nodes.data['deg'].expand_dims(1)\n",
        "        h_concat = nd.concat(h, h_agg / nd.maximum(deg, 1e-6), dim=1)\n",
        "        h_new = self.leaky_relu(self.W(h_concat))\n",
        "        # Layer norm\n",
        "        return {'h': h_new / nd.maximum(h_new.norm(axis=1, keepdims=True), 1e-6)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C03MxgOFuf1z"
      },
      "source": [
        "## Compute node embeddings on the MovieLens dataset\n",
        "\n",
        "User nodes and movie nodes have different sets of features. Thus, we need to generate embeddings differently.\n",
        "\n",
        "User nodes have categorial features of \"age\", \"gender\", \"occupation\" and \"zip code\". These features are all one-hot encodings. In addition, we add one-hot encoding for every user node. To generate user embedding, we add all of these embeddings together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbE4Cvffuf10"
      },
      "source": [
        "class UserEmbedding(nn.Block):\n",
        "    def __init__(self, G, feature_size, node_emb):\n",
        "        super(UserEmbedding, self).__init__()\n",
        "\n",
        "        # Embedding matrices for one-hot encoding.\n",
        "        self.emb_age = nn.Embedding(G.ndata['age'].max().asscalar() + 1,\n",
        "                                    feature_size)\n",
        "        self.emb_gender = nn.Embedding(G.ndata['gender'].max().asscalar() + 1,\n",
        "                                       feature_size)\n",
        "        self.emb_occupation = nn.Embedding(G.ndata['occupation'].max().asscalar() + 1,\n",
        "                                           feature_size)\n",
        "        self.emb_zip = nn.Embedding(G.ndata['zip'].max().asscalar() + 1,\n",
        "                                    feature_size)\n",
        "        \n",
        "        # One-hot encoding for each node.\n",
        "        self.node_emb = node_emb\n",
        "\n",
        "    def forward(self, ndata, nid):\n",
        "        h = self.node_emb(nid + 1)\n",
        "        extra_repr = []\n",
        "        extra_repr.append(self.emb_age(ndata['age']))\n",
        "        extra_repr.append(self.emb_gender(ndata['gender']))\n",
        "        extra_repr.append(self.emb_occupation(ndata['occupation']))\n",
        "        extra_repr.append(self.emb_zip(ndata['zip']))\n",
        "        return h + nd.stack(*extra_repr, axis=0).sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpKYdfppuf15"
      },
      "source": [
        "Movie nodes \"year\", \"genre\". \"year\" is one-hot encoding, \"genre\" is stored in a float32 dense matrix. Like user nodes, we add one-hot encoding to every movie node. To generate movie embedding, we add all of these embeddings together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfODTqPFuf17"
      },
      "source": [
        "class MovieEmbedding(nn.Block):\n",
        "    def __init__(self, G, feature_size, node_emb):\n",
        "        super(MovieEmbedding, self).__init__()\n",
        "        self.emb_year = nn.Embedding(G.ndata['year'].max().asscalar() + 1,\n",
        "                                     feature_size)\n",
        "\n",
        "        # Linear projection for float32 features.\n",
        "        seq = nn.Sequential()\n",
        "        with seq.name_scope():\n",
        "            seq.add(nn.Dense(feature_size))\n",
        "            seq.add(nn.LeakyReLU(0.1))\n",
        "        self.proj_genre = seq\n",
        "\n",
        "        # One-hot encoding for each node.\n",
        "        self.node_emb = node_emb\n",
        "\n",
        "    def forward(self, ndata, nid):\n",
        "        h = self.node_emb(nid + 1)\n",
        "        extra_repr = []\n",
        "        extra_repr.append(self.emb_year(ndata['year']))\n",
        "        extra_repr.append(self.proj_genre(ndata['genre']))\n",
        "        return h + nd.stack(*extra_repr, axis=0).sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB57tIhUuf2A"
      },
      "source": [
        "## Rating prediction\n",
        "\n",
        "For recommendation, the rating on item $j$ by user $i$ is defined by $u_i^T v_j$.\n",
        "\n",
        "In practice, recommendation models have user bias term and movie bias term: $u_i^T v_j + b_{u_i} + b_{v_j}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKaqpITfuf2B"
      },
      "source": [
        "class DotDecoder(nn.Block):\n",
        "    def __init__(self, num_nodes):\n",
        "        super(DotDecoder, self).__init__()\n",
        "        \n",
        "        with self.name_scope():\n",
        "            self.biases = self.params.get(\n",
        "                'node_biases',\n",
        "                init=mx.init.Zero(),\n",
        "                shape=(num_nodes+1,))\n",
        "            \n",
        "    def forward(self, h_users, h_items, users, items):\n",
        "        user_biases = self.biases.data()[users+1]\n",
        "        item_biases = self.biases.data()[items+1]\n",
        "        return (h_users * h_items).sum(1) + user_biases + item_biases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXjPStzguf2F"
      },
      "source": [
        "## Get the training set\n",
        "\n",
        "We use 80% of edges for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXGho2KCuf2G",
        "outputId": "f24175ad-d944-4476-a09f-8ae1f5bb8d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "g_train = ml.g_train\n",
        "rating_train = g_train.edata['rating']\n",
        "src_train, dst_train = g_train.all_edges()\n",
        "print('#vertices:', g_train.number_of_nodes())\n",
        "print('#edges:', g_train.number_of_edges())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#vertices: 2625\n",
            "#edges: 161940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMiNlsmRuf2M"
      },
      "source": [
        "## Get the testing edge set\n",
        "\n",
        "We use 20% of edges for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No2HFVeBuf2N",
        "outputId": "6231a25f-115b-434b-b624-6bad6b6f8d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "g = ml.g\n",
        "eid_test = g.filter_edges(lambda edges: edges.data['test']).astype('int64')\n",
        "src_test, dst_test = g.find_edges(eid_test)\n",
        "rating_test = g.edges[eid_test].data['rating']\n",
        "print('#testing edges:', len(eid_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#testing edges: 38060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vU-3hStuf2R"
      },
      "source": [
        "## Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "nbdC7l8cuf2S",
        "outputId": "6d95bdd0-7375-4633-fa6b-3ea3657986de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# We use 1-layer GraphSage as the graph encoder.\n",
        "# EXERCISE: Interested users can try different numbers of layers (e.g., 0, 1, 2)\n",
        "model = GNNRecommender(GraphSageEncoder(embedding_size=100, n_layers=1, G=g_train),\n",
        "                       DotDecoder(g_train.number_of_nodes()))\n",
        "model.collect_params().initialize(ctx=mx.cpu())\n",
        "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 0.003, 'wd': 1e-5})\n",
        "\n",
        "for epoch in range(200):\n",
        "    # Training\n",
        "    for _ in range(10):\n",
        "        with mx.autograd.record():\n",
        "            score = model(g_train, src_train, dst_train)\n",
        "            loss = ((score - rating_train) ** 2).mean()\n",
        "            loss.backward()\n",
        "        trainer.step(1)\n",
        "\n",
        "    # Testing\n",
        "    h = model.encoder(g)\n",
        "    # Compute test RMSE\n",
        "    score = model.decoder(h[src_test], h[dst_test], src_test, dst_test)\n",
        "    test_rmse = nd.sqrt(((score - rating_test) ** 2).mean())\n",
        "\n",
        "    print('Training loss:', loss.asscalar(), 'Test RMSE:', test_rmse.asscalar())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 7.425549 Test RMSE: 2.7286336\n",
            "Training loss: 7.1118746 Test RMSE: 2.6723719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8bfcd3b01f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrating_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-47d3072968be>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, G, users, items)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mh_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mh_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-2d584640351b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, G)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# The node embeddings computed by GraphSage layers are stored in node data of 'h'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-440213a0571a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, G)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# The message function and message reduce function used by GraphSage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# EXERCISE: Interested users can try different message functions and reduce functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_src\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'h_agg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Step 2 and 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/graph.py\u001b[0m in \u001b[0;36mupdate_all\u001b[0;34m(self, message_func, reduce_func, apply_node_func)\u001b[0m\n\u001b[1;32m   3236\u001b[0m                                           \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3237\u001b[0m                                           apply_func=apply_node_func)\n\u001b[0;32m-> 3238\u001b[0;31m             \u001b[0mRuntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m     def prop_nodes(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/runtime/runtime.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(prog)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# prog.pprint_exe(exe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mexe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/runtime/ir/executor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1198\u001b[0m         self.ret.data = F.copy_reduce(\n\u001b[1;32m   1199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreducer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             out_map)\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/backend/mxnet/tensor.py\u001b[0m in \u001b[0;36mcopy_reduce\u001b[0;34m(reducer, graph, target, in_data, out_size, in_map, out_map)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 out_map=(None, None)):\n\u001b[1;32m    520\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCopyReduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreducer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/autograd.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mprev_recording\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0mset_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_recording\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/backend/mxnet/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_data)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreducer\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreducer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_data_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_data_nd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             self.in_map[0], self.out_map[0])\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0;31m# normalize if mean reducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;31m# NOTE(zihao): this is a temporary hack and we should have better solution in the future.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/kernel.py\u001b[0m in \u001b[0;36mcopy_reduce\u001b[0;34m(reducer, G, target, X, out, X_rows, out_rows)\u001b[0m\n\u001b[1;32m    370\u001b[0m     _CAPI_DGLKernelCopyReduce(\n\u001b[1;32m    371\u001b[0m         \u001b[0mreducer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         X, out, X_rows, out_rows)\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;31m# pylint: disable=invalid-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/_ffi/_ctypes/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    188\u001b[0m         check_call(_LIB.DGLFuncCall(\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             ctypes.byref(ret_val), ctypes.byref(ret_tcode)))\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrMGlVmuuf2X"
      },
      "source": [
        "if 'h' in g_train.ndata:\n",
        "    del g_train.ndata['h']\n",
        "if 'h_agg' in g_train.ndata:\n",
        "    del g_train.ndata['h_agg']\n",
        "if 'h' in g.ndata:\n",
        "    del g.ndata['h']\n",
        "if 'h_agg' in g.ndata:\n",
        "    del g.ndata['h_agg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tkmir0Ruf2f"
      },
      "source": [
        "# Take-home Exercise\n",
        "\n",
        "Interested users can try different things given this simple recommendation model with GraphSage. Interesting experiments include:\n",
        "* try different numbers of GraphSage layers.\n",
        "* try different graph encoders. For example, replace the message passing functions with the one in [GAT](https://github.com/dmlc/dgl/blob/master/examples/mxnet/gat/gat.py) or GCMC.\n",
        "* try mini-batch training. A tutorial of training GraphSage with mini-batch has been included in the same folder of this tutorial.\n",
        "* try different recommendation datasets. To explore the benefit of graph neural networks, we suggest users to try feature-rich datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ5ORP5Vuf2g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}