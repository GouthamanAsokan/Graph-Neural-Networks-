{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Sampling_MX.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCzETGrIIdoQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "313d223c-8d42-499f-f02d-6d643d0e012a"
      },
      "source": [
        "!pip install dgl\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/b4/84e4ebd70ef3985181ef5d2d2a366a45af0e3cd18d249fb212ac03f683cf/dgl-0.4.3.post2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (2020.6.20)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.4.3.post2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYEeABHhIyhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "10982ca1-c722-49b7-ee7c-b0dc37569429"
      },
      "source": [
        "!pip install mxnet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
            "\u001b[K     |████████████████████████████████| 68.7MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.18.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GoILFWnIdoY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "NodeFlow and Sampling\n",
        "=======================================\n",
        "\n",
        "**Author**: Ziyue Huang, Da Zheng, Quan Gan, Jinjing Zhou, Zheng Zhang\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyiDH1KTIdoZ",
        "colab_type": "text"
      },
      "source": [
        "Graph convolutional network\n",
        "~~~\n",
        "\n",
        "In an $L$-layer graph convolution network (GCN), given a graph\n",
        "$G=(V, E)$, represented as an adjacency matrix $A$, with\n",
        "node features $H^{(0)} = X \\in \\mathbb{R}^{|V| \\times d}$, the\n",
        "hidden feature of a node $v$ in $(l+1)$-th layer\n",
        "$h_v^{(l+1)}$ depends on the features of all its neighbors in the\n",
        "previous layer $h_u^{(l)}$:\n",
        "\n",
        "\\begin{align}z_v^{(l+1)} = \\sum_{u \\in \\mathcal{N}(v)} \\tilde{A}_{uv} h_u^{(l)} \\qquad h_v^{(l+1)} = \\sigma ( z_v^{(l+1)} W^{(l)})\\end{align}\n",
        "\n",
        "where $\\mathcal{N}(v)$ is the neighborhood of $v$,\n",
        "$\\tilde{A}$ could be any normalized version of $A$ such as\n",
        "$D^{-1} A$ in Kipf et al., $\\sigma(\\cdot)$ is an activation\n",
        "function, and $W^{(l)}$ is a trainable parameter of the\n",
        "$l$-th layer.\n",
        "\n",
        "In the node classification task you minimize the following loss:\n",
        "\n",
        "\\begin{align}\\frac{1}{\\vert \\mathcal{V}_\\mathcal{L} \\vert} \\sum_{v \\in \\mathcal{V}_\\mathcal{L}} f(y_v, z_v^{(L)})\\end{align}\n",
        "\n",
        "where $y_v$ is the label of $v$, and $f(\\cdot, \\cdot)$\n",
        "is a loss function, e.g., cross entropy loss.\n",
        "\n",
        "While training GCN on the full graph, each node aggregates the hidden\n",
        "features of its neighbors to compute its hidden feature in the next\n",
        "layer.\n",
        "\n",
        "In this tutorial, you run GCN on the Reddit dataset constructed by `Hamilton et\n",
        "al. <https://arxiv.org/abs/1706.02216>`__, wherein the nodes are posts\n",
        "and edges are established if two nodes are commented by a same user. The\n",
        "task is to predict the category that a post belongs to. This graph has\n",
        "233,000 nodes, 114.6 million edges and 41 categories. First load the Reddit graph.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYN4JIsZIdoa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "7a8b35d5-2397-4d0b-bca7-2c4326fdd4b5"
      },
      "source": [
        "import numpy as np\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "from dgl import DGLGraph\n",
        "from dgl.data import RedditDataset\n",
        "import mxnet as mx\n",
        "from mxnet import gluon\n",
        "\n",
        "# Load MXNet as backend\n",
        "dgl.load_backend('mxnet')\n",
        "\n",
        "# load dataset\n",
        "data = RedditDataset(self_loop=True)\n",
        "train_nid = mx.nd.array(np.nonzero(data.train_mask)[0]).astype(np.int64)\n",
        "features = mx.nd.array(data.features)\n",
        "in_feats = features.shape[1]\n",
        "labels = mx.nd.array(data.labels)\n",
        "n_classes = data.num_labels\n",
        "\n",
        "# construct DGLGraph and prepare related data\n",
        "g = DGLGraph(data.graph, readonly=True)\n",
        "g.ndata['features'] = features"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using backend: mxnet\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.dgl/reddit_self_loop.zip from https://data.dgl.ai/dataset/reddit_self_loop.zip...\n",
            "Extracting file to /root/.dgl/reddit_self_loop\n",
            "Finished data loading.\n",
            "  NumNodes: 232965\n",
            "  NumEdges: 114848857\n",
            "  NumFeats: 602\n",
            "  NumClasses: 41\n",
            "  NumTrainingSamples: 153431\n",
            "  NumValidationSamples: 23831\n",
            "  NumTestSamples: 55703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHbXdKvsIdof",
        "colab_type": "text"
      },
      "source": [
        "Here you define the node UDF, which has a fully-connected layer:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_mZnvkMIdog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NodeUpdate(gluon.Block):\n",
        "    def __init__(self, in_feats, out_feats, activation=None):\n",
        "        super(NodeUpdate, self).__init__()\n",
        "        self.dense = gluon.nn.Dense(out_feats, in_units=in_feats)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, node):\n",
        "        h = node.data['h']\n",
        "        h = self.dense(h)\n",
        "        if self.activation:\n",
        "            h = self.activation(h)\n",
        "        return {'activation': h}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWi9FlT1Idol",
        "colab_type": "text"
      },
      "source": [
        "In DGL, you implement GCN on the full graph with ``update_all`` in ``DGLGraph``.\n",
        "The following code performs two-layer GCN on the Reddit graph.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aArnNXLlIdom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a880b69f-bd45-4968-fb41-f7a7fb332283"
      },
      "source": [
        "# number of GCN layers\n",
        "L = 2\n",
        "# number of hidden units of a fully connected layer\n",
        "n_hidden = 64\n",
        "\n",
        "layers = [NodeUpdate(g.ndata['features'].shape[1], n_hidden, mx.nd.relu),\n",
        "          NodeUpdate(n_hidden, n_hidden, mx.nd.relu)]\n",
        "for layer in layers:\n",
        "    layer.initialize()\n",
        "\n",
        "h = g.ndata['features']\n",
        "for i in range(L):\n",
        "    g.ndata['h'] = h\n",
        "    g.update_all(message_func=fn.copy_src(src='h', out='m'),\n",
        "                 reduce_func=fn.sum(msg='m', out='h'),\n",
        "                 apply_node_func=lambda node: {'h': layers[i](node)['activation']})\n",
        "    h = g.ndata.pop('h')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7b8e78608891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     g.update_all(message_func=fn.copy_src(src='h', out='m'),\n\u001b[1;32m     15\u001b[0m                  \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                  apply_node_func=lambda node: {'h': layers[i](node)['activation']})\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/graph.py\u001b[0m in \u001b[0;36mupdate_all\u001b[0;34m(self, message_func, reduce_func, apply_node_func)\u001b[0m\n\u001b[1;32m   3236\u001b[0m                                           \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3237\u001b[0m                                           apply_func=apply_node_func)\n\u001b[0;32m-> 3238\u001b[0;31m             \u001b[0mRuntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m     def prop_nodes(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/runtime/runtime.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(prog)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# prog.pprint_exe(exe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mexe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/runtime/ir/executor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0min_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dgl_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m         \u001b[0min_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0mout_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/graph.py\u001b[0m in \u001b[0;36mget_immutable_gidx\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m   4334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_immutable_gidx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_immutable_gidx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbits_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 '-'.join([str(k) + ':' + str(v) for k, v in kwargs.items()]))\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/graph_index.py\u001b[0m in \u001b[0;36mget_immutable_gidx\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mGraphIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \"\"\"\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_immutable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbits_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_csr_shuffle_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/graph_index.py\u001b[0m in \u001b[0;36masbits\u001b[0;34m(self, bits)\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mstored\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \"\"\"\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_CAPI_DGLImmutableGraphAsNumBits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mregister_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'graph.Subgraph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/_ffi/_ctypes/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    188\u001b[0m         check_call(_LIB.DGLFuncCall(\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             ctypes.byref(ret_val), ctypes.byref(ret_tcode)))\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RBTCmP8Idor",
        "colab_type": "text"
      },
      "source": [
        "NodeFlow\n",
        "~~~~~~~~~~~~~~~~~\n",
        "\n",
        "As the graph scales up to billions of nodes or edges, training on the\n",
        "full graph would no longer be efficient or even feasible.\n",
        "\n",
        "Mini-batch training allows you to control the computation and memory\n",
        "usage within some budget. The training loss for each iteration is\n",
        "\n",
        "\\begin{align}\\frac{1}{\\vert \\tilde{\\mathcal{V}}_\\mathcal{L} \\vert} \\sum_{v \\in \\tilde{\\mathcal{V}}_\\mathcal{L}} f(y_v, z_v^{(L)})\\end{align}\n",
        "\n",
        "where $\\tilde{\\mathcal{V}}_\\mathcal{L}$ is a subset sampled from\n",
        "the total labeled nodes $\\mathcal{V}_\\mathcal{L}$ uniformly at\n",
        "random.\n",
        "\n",
        "Stemming from the labeled nodes $\\tilde{\\mathcal{V}}_\\mathcal{L}$\n",
        "in a mini-batch and tracing back to the input forms a computational\n",
        "dependency graph (a directed acyclic graph [DAG]), which\n",
        "captures the computation flow of $Z^{(L)}$.\n",
        "\n",
        "In the example below, a mini-batch to compute the hidden features of\n",
        "node D in layer 2 requires hidden features of A, B, E, G in layer 1,\n",
        "which in turn requires hidden features of C, D, F in layer 0.\n",
        "\n",
        "|image0|\n",
        "\n",
        "For that purpose, you define ``NodeFlow`` to represent this computation\n",
        "flow.\n",
        "\n",
        "``NodeFlow`` is a type of layered graph, where nodes are organized in\n",
        "$L + 1$ sequential *layers*, and edges only exist between adjacent\n",
        "layers, forming *blocks*. You construct ``NodeFlow`` backwards, starting\n",
        "from the last layer with all the nodes whose hidden features are\n",
        "requested. The set of nodes the next layer depends on forms the previous\n",
        "layer. An edge connects a node in the previous layer to another in the\n",
        "next layer if the latter depends on the former. Repeat such process\n",
        "until all $L + 1$ layers are constructed. The feature of nodes in\n",
        "each layer, and that of edges in each block, are stored as separate\n",
        "tensors.\n",
        "\n",
        ".. raw:: html\n",
        "\n",
        "``NodeFlow`` provides ``block_compute`` for per-block computation, which\n",
        "triggers computation and data propogation from the lower layer to the\n",
        "next upper layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKxc2EnHIdos",
        "colab_type": "text"
      },
      "source": [
        "Neighbor sampling\n",
        "~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Real-world graphs often have nodes with large degree, meaning that a\n",
        "moderately deep (e.g., three layers) GCN would often depend on input features\n",
        "of the entire graph, even if the computation only depends on outputs of\n",
        "a few nodes, hence its cost-ineffectiveness.\n",
        "\n",
        "Sampling methods mitigate this computational problem by reducing the\n",
        "receptive field effectively. Fig-c above shows one such example.\n",
        "\n",
        "Instead of using all the $L$-hop neighbors of a node $v$,\n",
        "`Hamilton et al. <https://arxiv.org/abs/1706.02216>`__ propose *neighbor\n",
        "sampling*, which randomly samples a few neighbors\n",
        "$\\hat{\\mathcal{N}}^{(l)}(v)$ to estimate the aggregation\n",
        "$z_v^{(l+1)}$ of its total neighbors $\\mathcal{N}(v)$ in\n",
        "$l$-th GCN layer, by an unbiased estimator\n",
        "$\\hat{z}_v^{(l+1)}$\n",
        "\n",
        "\\begin{align}\\hat{z}_v^{(l+1)} = \\frac{\\vert \\mathcal{N}(v) \\vert }{\\vert \\hat{\\mathcal{N}}^{(l)}(v) \\vert} \\sum_{u \\in \\hat{\\mathcal{N}}^{(l)}(v)} \\tilde{A}_{uv} \\hat{h}_u^{(l)} \\qquad\n",
        "   \\hat{h}_v^{(l+1)} = \\sigma ( \\hat{z}_v^{(l+1)} W^{(l)} )\\end{align}\n",
        "\n",
        "Let $D^{(l)}$ be the number of neighbors to be sampled for each\n",
        "node at the $l$-th layer, then the receptive field size of each\n",
        "node can be controlled under $\\prod_{i=0}^{L-1} D^{(l)}$ by\n",
        "*neighbor sampling*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m2I6ojXIdot",
        "colab_type": "text"
      },
      "source": [
        "You then implement *neighbor sampling* by ``NodeFlow``:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST9PDDQAIdou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GCNSampling(gluon.Block):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 n_hidden,\n",
        "                 n_classes,\n",
        "                 n_layers,\n",
        "                 activation,\n",
        "                 dropout,\n",
        "                 **kwargs):\n",
        "        super(GCNSampling, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.n_layers = n_layers\n",
        "        with self.name_scope():\n",
        "            self.layers = gluon.nn.Sequential()\n",
        "            # input layer\n",
        "            self.layers.add(NodeUpdate(in_feats, n_hidden, activation))\n",
        "            # hidden layers\n",
        "            for i in range(1, n_layers-1):\n",
        "                self.layers.add(NodeUpdate(n_hidden, n_hidden, activation))\n",
        "            # output layer\n",
        "            self.layers.add(NodeUpdate(n_hidden, n_classes))\n",
        "\n",
        "    def forward(self, nf):\n",
        "        nf.layers[0].data['activation'] = nf.layers[0].data['features']\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = nf.layers[i].data.pop('activation')\n",
        "            if self.dropout:\n",
        "                h = mx.nd.Dropout(h, p=self.dropout)\n",
        "            nf.layers[i].data['h'] = h\n",
        "            # block_compute() computes the feature of layer i given layer\n",
        "            # i-1, with the given message, reduce, and apply functions.\n",
        "            # Here, you essentially aggregate the neighbor node features in\n",
        "            # the previous layer, and update it with the `layer` function.\n",
        "            nf.block_compute(i,\n",
        "                             fn.copy_src(src='h', out='m'),\n",
        "                             lambda node : {'h': node.mailbox['m'].mean(axis=1)},\n",
        "                             layer)\n",
        "        h = nf.layers[-1].data.pop('activation')\n",
        "        return h"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5doR-0BBIdoz",
        "colab_type": "text"
      },
      "source": [
        "DGL provides ``NeighborSampler`` to construct the ``NodeFlow`` for a\n",
        "mini-batch according to the computation logic of neighbor sampling.\n",
        "``NeighborSampler``\n",
        "returns an iterator that generates a ``NodeFlow`` each time. This function\n",
        "has many options to give users opportunities to customize the behavior\n",
        "of the neighbor sampler, including the number of neighbors to sample or\n",
        "the number of hops to sample, for example. Please see `its API\n",
        "document <https://doc.dgl.ai/api/python/sampler.html>`__ for more\n",
        "details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzlFuusWIdoz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "60a56ed6-703f-4f86-d2ef-e8f86cdc6698"
      },
      "source": [
        "# dropout probability\n",
        "dropout = 0.2\n",
        "# batch size\n",
        "batch_size = 1000\n",
        "# number of neighbors to sample\n",
        "num_neighbors = 4\n",
        "# number of epochs\n",
        "num_epochs = 1\n",
        "\n",
        "# initialize the model and cross entropy loss\n",
        "model = GCNSampling(in_feats, n_hidden, n_classes, L,\n",
        "                    mx.nd.relu, dropout, prefix='GCN')\n",
        "model.initialize()\n",
        "loss_fcn = gluon.loss.SoftmaxCELoss()\n",
        "\n",
        "# use adam optimizer\n",
        "trainer = gluon.Trainer(model.collect_params(), 'adam',\n",
        "                        {'learning_rate': 0.03, 'wd': 0})\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    i = 0\n",
        "    for nf in dgl.contrib.sampling.NeighborSampler(g, batch_size,\n",
        "                                                   num_neighbors,\n",
        "                                                   neighbor_type='in',\n",
        "                                                   shuffle=True,\n",
        "                                                   num_hops=L,\n",
        "                                                   seed_nodes=train_nid):\n",
        "        # When `NodeFlow` is generated from `NeighborSampler`, it only contains\n",
        "        # the topology structure, on which there is no data attached.\n",
        "        # Users need to call `copy_from_parent` to copy specific data,\n",
        "        # such as input node features, from the original graph.\n",
        "        nf.copy_from_parent()\n",
        "        with mx.autograd.record():\n",
        "            # forward\n",
        "            pred = model(nf)\n",
        "            batch_nids = nf.layer_parent_nid(-1).astype('int64')\n",
        "            batch_labels = labels[batch_nids]\n",
        "            # cross entropy loss\n",
        "            loss = loss_fcn(pred, batch_labels)\n",
        "            loss = loss.sum() / len(batch_nids)\n",
        "        # backward\n",
        "        loss.backward()\n",
        "        # optimization\n",
        "        trainer.step(batch_size=1)\n",
        "        print(\"Epoch[{}]: loss {}\".format(epoch, loss.asscalar()))\n",
        "        i += 1\n",
        "        # You only train the model with 32 mini-batches just for demonstration.\n",
        "        if i >= 32:\n",
        "            break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch[0]: loss 3.7311513423919678\n",
            "Epoch[0]: loss 2.9846324920654297\n",
            "Epoch[0]: loss 2.5716474056243896\n",
            "Epoch[0]: loss 2.2514915466308594\n",
            "Epoch[0]: loss 1.953652262687683\n",
            "Epoch[0]: loss 1.7096182107925415\n",
            "Epoch[0]: loss 1.5976336002349854\n",
            "Epoch[0]: loss 1.489196538925171\n",
            "Epoch[0]: loss 1.3743089437484741\n",
            "Epoch[0]: loss 1.2588484287261963\n",
            "Epoch[0]: loss 1.3931238651275635\n",
            "Epoch[0]: loss 1.3072576522827148\n",
            "Epoch[0]: loss 1.4125165939331055\n",
            "Epoch[0]: loss 1.157963752746582\n",
            "Epoch[0]: loss 1.1522687673568726\n",
            "Epoch[0]: loss 1.1378711462020874\n",
            "Epoch[0]: loss 1.1652178764343262\n",
            "Epoch[0]: loss 1.0647339820861816\n",
            "Epoch[0]: loss 0.9470056295394897\n",
            "Epoch[0]: loss 0.993992269039154\n",
            "Epoch[0]: loss 1.188017725944519\n",
            "Epoch[0]: loss 0.9760155081748962\n",
            "Epoch[0]: loss 1.0203027725219727\n",
            "Epoch[0]: loss 1.0055545568466187\n",
            "Epoch[0]: loss 1.0037815570831299\n",
            "Epoch[0]: loss 0.9230003952980042\n",
            "Epoch[0]: loss 0.9360195994377136\n",
            "Epoch[0]: loss 0.9567748308181763\n",
            "Epoch[0]: loss 0.9225091934204102\n",
            "Epoch[0]: loss 1.0345141887664795\n",
            "Epoch[0]: loss 0.9235912561416626\n",
            "Epoch[0]: loss 0.9204977750778198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YkcG8XuIdo6",
        "colab_type": "text"
      },
      "source": [
        "Control variate\n",
        "~~~~~~~~~~~~~~~\n",
        "\n",
        "The unbiased estimator $\\hat{Z}^{(\\cdot)}$ used in *neighbor\n",
        "sampling* might suffer from high variance, so it still requires a\n",
        "relatively large number of neighbors, e.g. \\ $D^{(0)}=25$ and\n",
        "$D^{(1)}=10$ in `Hamilton et\n",
        "al. <https://arxiv.org/abs/1706.02216>`__. With *control variate*, a\n",
        "standard variance reduction technique widely used in Monte Carlo\n",
        "methods, 2 neighbors for a node seems sufficient.\n",
        "\n",
        "*Control variate* method works as follows: Given a random variable\n",
        "$X$ and you wish to estimate its expectation\n",
        "$\\mathbb{E} [X] = \\theta$, it finds another random variable\n",
        "$Y$ which is highly correlated with $X$ and whose\n",
        "expectation $\\mathbb{E} [Y]$ can be easily computed. The *control\n",
        "variate* estimator $\\tilde{X}$ is\n",
        "\n",
        "\\begin{align}\\tilde{X} = X - Y + \\mathbb{E} [Y] \\qquad \\mathbb{VAR} [\\tilde{X}] = \\mathbb{VAR} [X] + \\mathbb{VAR} [Y] - 2 \\cdot \\mathbb{COV} [X, Y]\\end{align}\n",
        "\n",
        "If $\\mathbb{VAR} [Y] - 2\\mathbb{COV} [X, Y] < 0$, then\n",
        "$\\mathbb{VAR} [\\tilde{X}] < \\mathbb{VAR} [X]$.\n",
        "\n",
        "`Chen et al. <https://arxiv.org/abs/1710.10568>`__ proposed a *control\n",
        "variate* based estimator used in GCN training, by using history\n",
        "$\\bar{H}^{(l)}$ of the nodes which are not sampled, the modified\n",
        "estimator $\\hat{z}_v^{(l+1)}$ is\n",
        "\n",
        "\\begin{align}\\hat{z}_v^{(l+1)} = \\frac{\\vert \\mathcal{N}(v) \\vert }{\\vert \\hat{\\mathcal{N}}^{(l)}(v) \\vert} \\sum_{u \\in \\hat{\\mathcal{N}}^{(l)}(v)} \\tilde{A}_{uv} ( \\hat{h}_u^{(l)} - \\bar{h}_u^{(l)} ) + \\sum_{u \\in \\mathcal{N}(v)} \\tilde{A}_{uv} \\bar{h}_u^{(l)} \\\\\n",
        "   \\hat{h}_v^{(l+1)} = \\sigma ( \\hat{z}_v^{(l+1)} W^{(l)} )\\end{align}\n",
        "\n",
        "This method can also be *conceptually* implemented in DGL as shown\n",
        "here.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSBghd40Ido7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "have_large_memory = False\n",
        "# The control-variate sampling code below needs to run on a large-memory\n",
        "# machine for the Reddit graph.\n",
        "if have_large_memory:\n",
        "    g.ndata['h_0'] = features\n",
        "    for i in range(L):\n",
        "        g.ndata['h_{}'.format(i+1)] = mx.nd.zeros((features.shape[0], n_hidden))\n",
        "    # With control-variate sampling, you only need to sample two neighbors to train GCN.\n",
        "    for nf in dgl.contrib.sampling.NeighborSampler(g, batch_size, expand_factor=2,\n",
        "                                                   neighbor_type='in', num_hops=L,\n",
        "                                                   seed_nodes=train_nid):\n",
        "        for i in range(nf.num_blocks):\n",
        "            # aggregate history on the original graph\n",
        "            g.pull(nf.layer_parent_nid(i+1),\n",
        "                   fn.copy_src(src='h_{}'.format(i), out='m'),\n",
        "                   lambda node: {'agg_h_{}'.format(i): node.mailbox['m'].mean(axis=1)})\n",
        "        nf.copy_from_parent()\n",
        "        h = nf.layers[0].data['features']\n",
        "        for i in range(nf.num_blocks):\n",
        "            prev_h = nf.layers[i].data['h_{}'.format(i)]\n",
        "            # compute delta_h, the difference of the current activation and the history\n",
        "            nf.layers[i].data['delta_h'] = h - prev_h\n",
        "            # refresh the old history\n",
        "            nf.layers[i].data['h_{}'.format(i)] = h.detach()\n",
        "            # aggregate the delta_h\n",
        "            nf.block_compute(i,\n",
        "                             fn.copy_src(src='delta_h', out='m'),\n",
        "                             lambda node: {'delta_h': node.data['m'].mean(axis=1)})\n",
        "            delta_h = nf.layers[i + 1].data['delta_h']\n",
        "            agg_h = nf.layers[i + 1].data['agg_h_{}'.format(i)]\n",
        "            # control variate estimator\n",
        "            nf.layers[i + 1].data['h'] = delta_h + agg_h\n",
        "            nf.apply_layer(i + 1, lambda node : {'h' : layer(node.data['h'])})\n",
        "            h = nf.layers[i + 1].data['h']\n",
        "        # update history\n",
        "        nf.copy_to_parent()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-NoSqgDIdo_",
        "colab_type": "text"
      },
      "source": [
        "You can see full example here, `MXNet\n",
        "code <https://github.com/dmlc/dgl/blob/master/examples/mxnet/sampling/>`__\n",
        "and `PyTorch\n",
        "code <https://github.com/dmlc/dgl/tree/master/examples/pytorch/sampling>`__.\n",
        "\n",
        "Below shows the performance of graph convolution network and GraphSage\n",
        "with neighbor sampling and control variate sampling on the Reddit\n",
        "dataset. Our GraphSage with control variate sampling, when sampling one\n",
        "neighbor, can achieve over 96 percent test accuracy. |image1|\n",
        "\n",
        "More APIs\n",
        "~~~~~~~~~\n",
        "\n",
        "In fact, ``block_compute`` is one of the APIs that comes with\n",
        "``NodeFlow``, which provides flexibility to research new ideas. The\n",
        "computation flow underlying a DAG can be executed in one sweep, by\n",
        "calling ``prop_flows``.\n",
        "\n",
        "``prop_flows`` accepts a list of UDFs. The code below defines node update UDFs\n",
        "for each layer and computes a simplified version of GCN with neighbor sampling.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTZLCNscIdpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "apply_node_funcs = [\n",
        "    lambda node : {'h' : layers[0](node)['activation']},\n",
        "    lambda node : {'h' : layers[1](node)['activation']},\n",
        "]\n",
        "for nf in dgl.contrib.sampling.NeighborSampler(g, batch_size, num_neighbors,\n",
        "                                               neighbor_type='in', num_hops=L,\n",
        "                                               seed_nodes=train_nid):\n",
        "    nf.copy_from_parent()\n",
        "    nf.layers[0].data['h'] = nf.layers[0].data['features']\n",
        "    nf.prop_flow(fn.copy_src(src='h', out='m'),\n",
        "                 fn.sum(msg='m', out='h'), apply_node_funcs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvFDiYVWIdpE",
        "colab_type": "text"
      },
      "source": [
        "Internally, ``prop_flow`` triggers the computation by fusing together\n",
        "all the block computations, from the input to the top. The main\n",
        "advantages of this API are 1) simplicity, 2) allowing more system-level\n",
        "optimization in the future.\n",
        "\n",
        ".. |image0| image:: https://data.dgl.ai/tutorial/sampling/NodeFlow.png\n",
        ".. |image1| image:: https://data.dgl.ai/tutorial/sampling/sampling_result.png\n",
        "\n",
        "\n"
      ]
    }
  ]
}