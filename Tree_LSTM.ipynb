{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tree-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DJhA7dD_ta0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "9e93a53e-24a7-420d-eea8-2e6b2cf1bd6c"
      },
      "source": [
        "!pip install dgl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/b4/84e4ebd70ef3985181ef5d2d2a366a45af0e3cd18d249fb212ac03f683cf/dgl-0.4.3.post2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.4.3.post2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hbYxhJy_76X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "70f56a33-75c6-4375-fc20-ba2cebe34b20"
      },
      "source": [
        "#Importing the libraries\n",
        "import dgl\n",
        "from dgl.data.tree import SST\n",
        "from dgl.data import SSTBatch\n",
        "\n",
        "# Each sample in the dataset is a constituency tree. The leaf nodes\n",
        "# represent words. The word is an int value stored in the \"x\" field.\n",
        "# The non-leaf nodes have a special word PAD_WORD. The sentiment\n",
        "# label is stored in the \"y\" feature field.\n",
        "trainset = SST(mode='tiny')  # the \"tiny\" set has only five trees\n",
        "tiny_sst = trainset.trees\n",
        "num_vocabs = trainset.num_vocabs\n",
        "num_classes = trainset.num_classes\n",
        "\n",
        "vocab = trainset.vocab # vocabulary dict: key -> id\n",
        "inv_vocab = {v: k for k, v in vocab.items()} # inverted vocabulary dict: id -> word\n",
        "\n",
        "a_tree = tiny_sst[0]\n",
        "for token in a_tree.ndata['x'].tolist():\n",
        "    if token != trainset.PAD_WORD:\n",
        "        print(inv_vocab[token], end=\" \")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Downloading /root/.dgl/sst.zip from https://data.dgl.ai/dataset/sst.zip...\n",
            "Extracting file to /root/.dgl/sst\n",
            "Preprocessing...\n",
            "Dataset creation finished. #Trees: 5\n",
            "the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsKsSFiCCUxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "\n",
        "class TreeLSTMCell(nn.Module):\n",
        "    def __init__(self, x_size, h_size):\n",
        "        super(TreeLSTMCell, self).__init__()\n",
        "        self.W_iou = nn.Linear(x_size, 3 * h_size, bias=False)\n",
        "        self.U_iou = nn.Linear(2 * h_size, 3 * h_size, bias=False)\n",
        "        self.b_iou = nn.Parameter(th.zeros(1, 3 * h_size))\n",
        "        self.U_f = nn.Linear(2 * h_size, 2 * h_size)\n",
        "\n",
        "    def message_func(self, edges):\n",
        "        return {'h': edges.src['h'], 'c': edges.src['c']}\n",
        "\n",
        "    def reduce_func(self, nodes):\n",
        "        # concatenate h_jl for equation (1), (2), (3), (4)\n",
        "        h_cat = nodes.mailbox['h'].view(nodes.mailbox['h'].size(0), -1)\n",
        "\n",
        "        # equation (2)\n",
        "        f = th.sigmoid(self.U_f(h_cat)).view(*nodes.mailbox['h'].size())\n",
        "        # second term of equation (5)\n",
        "        c = th.sum(f * nodes.mailbox['c'], 1)\n",
        "        return {'iou': self.U_iou(h_cat), 'c': c}\n",
        "\n",
        "    def apply_node_func(self, nodes):\n",
        "        # equation (1), (3), (4)\n",
        "        iou = nodes.data['iou'] + self.b_iou\n",
        "        i, o, u = th.chunk(iou, 3, 1)\n",
        "        i, o, u = th.sigmoid(i), th.sigmoid(o), th.tanh(u)\n",
        "        # equation (5)\n",
        "        c = i * u + nodes.data['c']\n",
        "        # equation (6)\n",
        "        h = o * th.tanh(c)\n",
        "        return {'h' : h, 'c' : c}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6G28LZbqzmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "graph = dgl.batch(tiny_sst)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWPbNk9fCYVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "5b469312-ebd4-4d10-b3b0-c4166f9acb34"
      },
      "source": [
        "print('Traversing one tree:')\n",
        "print(dgl.topological_nodes_generator(a_tree))\n",
        "\n",
        "print('Traversing many trees at the same time:')\n",
        "print(dgl.topological_nodes_generator(graph))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traversing one tree:\n",
            "(tensor([ 2,  3,  6,  8, 13, 15, 17, 19, 22, 23, 25, 27, 28, 29, 30, 32, 34, 36,\n",
            "        38, 40, 43, 46, 47, 49, 50, 52, 58, 59, 60, 62, 64, 65, 66, 68, 69, 70]), tensor([ 1, 21, 26, 45, 48, 57, 63, 67]), tensor([24, 44, 56, 61]), tensor([20, 42, 55]), tensor([18, 54]), tensor([16, 53]), tensor([14, 51]), tensor([12, 41]), tensor([11, 39]), tensor([10, 37]), tensor([35]), tensor([33]), tensor([31]), tensor([9]), tensor([7]), tensor([5]), tensor([4]), tensor([0]))\n",
            "Traversing many trees at the same time:\n",
            "(tensor([  2,   3,   6,   8,  13,  15,  17,  19,  22,  23,  25,  27,  28,  29,\n",
            "         30,  32,  34,  36,  38,  40,  43,  46,  47,  49,  50,  52,  58,  59,\n",
            "         60,  62,  64,  65,  66,  68,  69,  70,  74,  76,  78,  79,  82,  83,\n",
            "         85,  88,  90,  92,  93,  95,  96, 100, 102, 103, 105, 109, 110, 112,\n",
            "        113, 117, 118, 119, 121, 125, 127, 129, 130, 132, 133, 135, 138, 140,\n",
            "        141, 142, 143, 150, 152, 153, 155, 158, 159, 161, 162, 164, 168, 170,\n",
            "        171, 174, 175, 178, 179, 182, 184, 185, 187, 189, 190, 191, 192, 195,\n",
            "        197, 198, 200, 202, 205, 208, 210, 212, 213, 214, 216, 218, 219, 220,\n",
            "        223, 225, 227, 229, 230, 232, 235, 237, 240, 242, 244, 246, 248, 249,\n",
            "        251, 253, 255, 256, 257, 259, 262, 263, 267, 269, 270, 271, 272]), tensor([  1,  21,  26,  45,  48,  57,  63,  67,  77,  81,  91,  94, 101, 108,\n",
            "        111, 116, 128, 131, 139, 151, 157, 160, 169, 173, 177, 183, 188, 196,\n",
            "        211, 217, 228, 247, 254, 261, 268]), tensor([ 24,  44,  56,  61,  75,  89,  99, 107, 115, 126, 137, 149, 156, 167,\n",
            "        181, 186, 194, 209, 215, 226, 245, 252, 266]), tensor([ 20,  42,  55,  73,  87, 124, 136, 154, 180, 207, 224, 243, 250, 265]), tensor([ 18,  54,  86, 123, 134, 148, 176, 206, 222, 241, 264]), tensor([ 16,  53,  84, 122, 172, 204, 239, 260]), tensor([ 14,  51,  80, 120, 166, 203, 238, 258]), tensor([ 12,  41,  72, 114, 165, 201, 236]), tensor([ 11,  39, 106, 163, 199, 234]), tensor([ 10,  37, 104, 147, 193, 233]), tensor([ 35,  98, 146, 231]), tensor([ 33,  97, 145, 221]), tensor([ 31,  71, 144]), tensor([9]), tensor([7]), tensor([5]), tensor([4]), tensor([0]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j749qG2XCvFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dgl.function as fn\n",
        "import torch as th\n",
        "\n",
        "graph.ndata['a'] = th.ones(graph.number_of_nodes(), 1)\n",
        "graph.register_message_func(fn.copy_src('a', 'a'))\n",
        "graph.register_reduce_func(fn.sum('a', 'a'))\n",
        "\n",
        "traversal_order = dgl.topological_nodes_generator(graph)\n",
        "graph.prop_nodes(traversal_order)\n",
        "\n",
        "# the following is a syntax sugar that does the same\n",
        "# dgl.prop_nodes_topo(graph)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOP9O6HHCwdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TreeLSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_vocabs,\n",
        "                 x_size,\n",
        "                 h_size,\n",
        "                 num_classes,\n",
        "                 dropout,\n",
        "                 pretrained_emb=None):\n",
        "        super(TreeLSTM, self).__init__()\n",
        "        self.x_size = x_size\n",
        "        self.embedding = nn.Embedding(num_vocabs, x_size)\n",
        "        if pretrained_emb is not None:\n",
        "            print('Using glove')\n",
        "            self.embedding.weight.data.copy_(pretrained_emb)\n",
        "            self.embedding.weight.requires_grad = True\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(h_size, num_classes)\n",
        "        self.cell = TreeLSTMCell(x_size, h_size)\n",
        "\n",
        "    def forward(self, batch, h, c):\n",
        "        \"\"\"Compute tree-lstm prediction given a batch.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch : dgl.data.SSTBatch\n",
        "            The data batch.\n",
        "        h : Tensor\n",
        "            Initial hidden state.\n",
        "        c : Tensor\n",
        "            Initial cell state.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        logits : Tensor\n",
        "            The prediction of each node.\n",
        "        \"\"\"\n",
        "        g = batch.graph\n",
        "        g.register_message_func(self.cell.message_func)\n",
        "        g.register_reduce_func(self.cell.reduce_func)\n",
        "        g.register_apply_node_func(self.cell.apply_node_func)\n",
        "        # feed embedding\n",
        "        embeds = self.embedding(batch.wordid * batch.mask)\n",
        "        g.ndata['iou'] = self.cell.W_iou(self.dropout(embeds)) * batch.mask.float().unsqueeze(-1)\n",
        "        g.ndata['h'] = h\n",
        "        g.ndata['c'] = c\n",
        "        # propagate\n",
        "        dgl.prop_nodes_topo(g)\n",
        "        # compute logits\n",
        "        h = self.dropout(g.ndata.pop('h'))\n",
        "        logits = self.linear(h)\n",
        "        return logits"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vUyrpY8C34t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f804a676-9362-4a13-a19b-032164ff532e"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = th.device('cpu')\n",
        "# hyper parameters\n",
        "x_size = 256\n",
        "h_size = 256\n",
        "dropout = 0.5\n",
        "lr = 0.05\n",
        "weight_decay = 1e-4\n",
        "epochs = 10\n",
        "\n",
        "# create the model\n",
        "model = TreeLSTM(trainset.num_vocabs,\n",
        "                 x_size,\n",
        "                 h_size,\n",
        "                 trainset.num_classes,\n",
        "                 dropout)\n",
        "print(model)\n",
        "\n",
        "# create the optimizer\n",
        "optimizer = th.optim.Adagrad(model.parameters(),\n",
        "                          lr=lr,\n",
        "                          weight_decay=weight_decay)\n",
        "\n",
        "def batcher(dev):\n",
        "    def batcher_dev(batch):\n",
        "        batch_trees = dgl.batch(batch)\n",
        "        return SSTBatch(graph=batch_trees,\n",
        "                        mask=batch_trees.ndata['mask'].to(device),\n",
        "                        wordid=batch_trees.ndata['x'].to(device),\n",
        "                        label=batch_trees.ndata['y'].to(device))\n",
        "    return batcher_dev\n",
        "\n",
        "train_loader = DataLoader(dataset=tiny_sst,\n",
        "                          batch_size=5,\n",
        "                          collate_fn=batcher(device),\n",
        "                          shuffle=False,\n",
        "                          num_workers=0)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        g = batch.graph\n",
        "        n = g.number_of_nodes()\n",
        "        h = th.zeros((n, h_size))\n",
        "        c = th.zeros((n, h_size))\n",
        "        logits = model(batch, h, c)\n",
        "        logp = F.log_softmax(logits, 1)\n",
        "        loss = F.nll_loss(logp, batch.label, reduction='sum')\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred = th.argmax(logits, 1)\n",
        "        acc = float(th.sum(th.eq(batch.label, pred))) / len(batch.label)\n",
        "        print(\"Epoch {:05d} | Step {:05d} | Loss {:.4f} | Acc {:.4f} |\".format(\n",
        "            epoch, step, loss.item(), acc))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TreeLSTM(\n",
            "  (embedding): Embedding(19536, 256)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (linear): Linear(in_features=256, out_features=5, bias=True)\n",
            "  (cell): TreeLSTMCell(\n",
            "    (W_iou): Linear(in_features=256, out_features=768, bias=False)\n",
            "    (U_iou): Linear(in_features=512, out_features=768, bias=False)\n",
            "    (U_f): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 00000 | Step 00000 | Loss 444.0598 | Acc 0.1392 |\n",
            "Epoch 00001 | Step 00000 | Loss 246.5826 | Acc 0.7546 |\n",
            "Epoch 00002 | Step 00000 | Loss 1180.7988 | Acc 0.3187 |\n",
            "Epoch 00003 | Step 00000 | Loss 342.3342 | Acc 0.5641 |\n",
            "Epoch 00004 | Step 00000 | Loss 245.3541 | Acc 0.8168 |\n",
            "Epoch 00005 | Step 00000 | Loss 142.6749 | Acc 0.8205 |\n",
            "Epoch 00006 | Step 00000 | Loss 102.7007 | Acc 0.9011 |\n",
            "Epoch 00007 | Step 00000 | Loss 101.7493 | Acc 0.8864 |\n",
            "Epoch 00008 | Step 00000 | Loss 90.7105 | Acc 0.8791 |\n",
            "Epoch 00009 | Step 00000 | Loss 71.3669 | Acc 0.9304 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}